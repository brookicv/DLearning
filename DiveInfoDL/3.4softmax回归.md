
## softmax回归模型

与线性回归不同的是，softmax回归输出多个数值（与标签类别的个数相同）。 假设有4种特征3个类别，则权重有12个，bias有3个，针对每个输入得到三个输出结果$o_1,o_2,o_3$，对应于3个类别。

$$
\begin{aligned}
o_1 &= x_1w_{11} + x_2w_{21} + x_3w_{31} + x_4w_{41} + b_1 \\
o_2 &= x_1w_{12} + x_2w_{22} + x_3 w_{32} + x_4w_{42} + b_2 \\
o_3 &= x_1 w_{13} + x_2w_{23} + x_3w_{33} + x_4w_{43} + b_3
\end{aligned}
$$


使用矩阵$X$表示训练数据集，在$X$中每一行代表一个训练样本，列数表示训练样本的特征个数。

权重矩阵$W$右乘于$X$，其每一列对应于一个输出节点对应于输入的样本的权重，例如上式中，第一个输出节点的输出结果为$o_1$，其表达式为
$$
o_1 = x_1w_{11} + x_2w_{21} + x_3w_{31} + x_4w_{41} + b_1
$$

其权重向量为$[w_{11},w_{21},w_{31},w_{41}]^T$。


分类问题的输出$o_i$可以当做某个类别的置信度，可以将置信度最大的类别作为结果输出。 由于置信度通常市固定在一个范围内（0,1）内，并且真实的类别的标签是离散值，可以使用`softma`函数来解决：

$$
\hat y_1,\hat y_2,\hat y_3 = softmax(o_1,o_2,o_3)
$$
其中
$$
\begin{aligned}
\hat y_1 &= \frac{exp(o_1)}{\sum_{i=1}^3 exp(o_i)} \\
\hat y_2 &= \frac{exp(o_2)}{\sum_{i=1}^3 exp(o_i)} \\
\hat y_3 &= \frac{exp(o_3)}{\sum_{i=1}^3 exp(o_i)} 
\end{aligned}
$$

这样有$\hat y_1 + \hat y_2 + \hat y_3 = 1$，且$\hat y_1 ,\hat y_2,\hat y_3 \in [0,1]$ ，是一个合法的概率分布。

在计算损失函数值的时候，要保证对应类别的输出$\hat y_i$最大即可。

综合以上，针对单个样本的$x^i$ 其计算如下

$$
\begin{aligned}
o_i &= x^i W + b\\
\hat y^i &= softmax(o_i)
\end{aligned}
$$

其中$W$为输出权重矩阵，每一列对应于一个节点。

## 交叉熵损失函数

softmax输出为每个类别的置信度，在计算损失值的时候，需要保证真实标签对应类别的置信度为最大即可。例如，在图像分类中，如果一个样本的真实标签为3，这时候只需要$\hat y_3$的值比其他两个类别的置信度$\hat y_1,\hat y_2$ 即可。 如果使用平均损失函数的话，要求过于严格。

为了解决softmax的损失问题，要引入一个**能够评估两个概率分布差异的度量函数**。交叉熵(cross entropy) 是一种常用的方法
$$
H(y,\hat y) = - \sum_{j=1}^q y_j \log \hat y_j 
$$

交叉熵只关心对正确类别的预测概率，只要其值足够大 就能够保证正确的分类结果。